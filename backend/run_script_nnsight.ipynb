{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from functools import lru_cache\n",
    "\n",
    "# nnsight / transformers imports\n",
    "import nnsight\n",
    "from nnsight import NNsight\n",
    "from nnsight import apply as nnsight_apply\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# custom SAE code\n",
    "from sae_lens import SAE\n",
    "\n",
    "################################################################################\n",
    "# 1. DATASET\n",
    "################################################################################\n",
    "\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    \"\"\"\n",
    "    Create a list of examples. Each example has a 'correct' and an 'error' dict lookup.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(num_samples):\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        # correct\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = (\n",
    "            'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f\">>> age = {age_dict}\\n>>> age[\\\"{correct_name}\\\"]\\n\"\n",
    "        )\n",
    "        correct_response = age_dict[correct_name]\n",
    "        # incorrect\n",
    "        incorrect_name = random.choice([n for n in name_pool if n not in age_dict])\n",
    "        if random.random() > 0.5:\n",
    "            incorrect_prompt = (\n",
    "                'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "                f\">>> age = {age_dict}\\n>>> age[\\\"{incorrect_name}\\\"]\\n\"\n",
    "            )\n",
    "        else:\n",
    "            keys = list(age_dict.keys())\n",
    "            items = list(age_dict.values())\n",
    "            location = keys.index(correct_name)\n",
    "            keys[location] = incorrect_name\n",
    "            broken_age_dict = dict(zip(keys, items))\n",
    "            incorrect_prompt = (\n",
    "                'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "                f\">>> age = {broken_age_dict}\\n>>> age[\\\"{correct_name}\\\"]\\n\"\n",
    "            )\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": \"Traceback\"\n",
    "            }\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "class ContrastiveDatasetBatch:\n",
    "    \"\"\"\n",
    "    - Takes a subset of items\n",
    "    - Tokenizes\n",
    "    - Stores last-token labels\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_items, tokenizer, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # Separate correct and error examples\n",
    "        self.correct_batch = [item[\"correct\"] for item in dataset_items]\n",
    "        self.error_batch   = [item[\"error\"]   for item in dataset_items]\n",
    "        self.batch_size    = len(self.correct_batch)\n",
    "\n",
    "        # Extract prompts\n",
    "        correct_prompts = [ex[\"prompt\"] for ex in self.correct_batch]\n",
    "        error_prompts   = [ex[\"prompt\"] for ex in self.error_batch]\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # Tokenize all prompts together\n",
    "        # ----------------------------------------\n",
    "        all_prompts = correct_prompts + error_prompts\n",
    "        self.all_tokenized = tokenizer(all_prompts, return_tensors=\"pt\", padding=True)['input_ids']\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # Tokenize correct prompts and error prompts separately\n",
    "        # ----------------------------------------\n",
    "        correct_tokenized = tokenizer(correct_prompts, return_tensors=\"pt\", padding=True)['input_ids']\n",
    "        error_tokenized   = tokenizer(error_prompts,   return_tensors=\"pt\", padding=True)['input_ids']\n",
    "\n",
    "        # Move to device\n",
    "        # self.correct_tokenized = {k: v.to(device) for k, v in correct_tokenized.items()}\n",
    "        # self.error_tokenized   = {k: v.to(device) for k, v in error_tokenized.items()}\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # Final non-pad index (last valid token)\n",
    "        # ----------------------------------------\n",
    "        # correct_mask = self.correct_tokenized['attention_mask']\n",
    "        # error_mask   = self.error_tokenized['attention_mask']\n",
    "\n",
    "        # self.correct_last_nonpad = correct_mask.sum(dim=-1) - 1  # shape [batch_size]\n",
    "        # self.error_last_nonpad   = error_mask.sum(dim=-1) - 1    # shape [batch_size]\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # Label extraction\n",
    "        # ----------------------------------------\n",
    "        def single_token_id(response_str):\n",
    "            # Convert response to a single token ID (or 2 tokens, we pick the second if possible)\n",
    "            t = tokenizer(str(response_str), return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "            if t.shape[1] >= 2:\n",
    "                return t[0, 1]\n",
    "            else:\n",
    "                return t[0, -1]\n",
    "\n",
    "        # Get responses for correct / error\n",
    "        correct_responses = [ex[\"response\"] for ex in self.correct_batch]\n",
    "        error_responses   = [ex[\"response\"] for ex in self.error_batch]\n",
    "\n",
    "        # Single-token labels for correct / error\n",
    "        correct_labels = [single_token_id(r) for r in correct_responses]\n",
    "        error_labels   = [single_token_id(r) for r in error_responses]\n",
    "\n",
    "        self.correct_labels = torch.tensor(correct_labels, dtype=torch.long, device=device)\n",
    "        self.error_labels   = torch.tensor(error_labels,   dtype=torch.long, device=device)\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # All labels in the same order as all_tokenized\n",
    "        # ----------------------------------------\n",
    "        all_responses = correct_responses + error_responses\n",
    "        all_labels    = [single_token_id(r) for r in all_responses]\n",
    "        self.all_labels = torch.tensor(all_labels, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 2. LOAD MODEL + SAES, AND WRAP WITH NNSIGHT\n",
    "################################################################################\n",
    "\n",
    "def load_model_and_saes():\n",
    "    \"\"\"\n",
    "    - Load a quantized model with bitsandbytes\n",
    "    - Wrap with NNsight\n",
    "    - Create SAEs (4 layers)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n",
    "    model_raw = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-9b\",\n",
    "        # quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Wrap with NNsight\n",
    "    wrapped_model = NNsight(model_raw)\n",
    "\n",
    "    # Example: 4 SAEs\n",
    "    layers = [7, 14, 21, 40]\n",
    "    l0s   = [92, 67, 129, 125]\n",
    "    saes  = []\n",
    "    for layer, l0_val in zip(layers, l0s):\n",
    "        # Adapt your SAE loading code as needed:\n",
    "        # e.g. SAE.from_pretrained(...)\n",
    "        # For demonstration, we create a dummy SAE below. Replace with real.\n",
    "        sae_obj = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-9b-pt-res\",\n",
    "            sae_id=f\"layer_{layer}/width_16k/average_l0_{l0_val}\",\n",
    "            device=device\n",
    "        )[0]\n",
    "        # freeze\n",
    "        for p in sae_obj.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        saes.append(sae_obj)\n",
    "\n",
    "    # Freeze the main model’s parameters\n",
    "    for param in model_raw.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "    sae_dict = {}\n",
    "    component_dict = {}\n",
    "    for i, layer in enumerate(layers):\n",
    "        component_dict[layer] = wrapped_model.model.layers[layer]\n",
    "        sae_dict[layer] = saes[i]\n",
    "\n",
    "\n",
    "    return tokenizer, wrapped_model, saes, sae_dict, component_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392490ad85384c8dbacbade8b574ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, wrapped_model, saes, sae_dict, component_dict = load_model_and_saes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\", \"Diego\"\n",
    "]\n",
    "full_dataset = generate_extended_dataset(extended_name_pool, num_samples=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {\\'Megan\\': 19, \\'Sam\\': 10, \\'Sally\\': 14, \\'Olivia\\': 19, \\'Emily\\': 14}\\n>>> age[\"Jackson\"]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[0]['error']['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_tuple(x):\n",
    "    assert isinstance(x, tuple), \"must be tuple tensor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_items = ContrastiveDatasetBatch(full_dataset[:30], tokenizer)\n",
    "\n",
    "# mean_ablation_dict = {}\n",
    "\n",
    "\n",
    "# with wrapped_model.trace(mean_items.all_tokenized):\n",
    "#     for key, value in component_dict.items():\n",
    "#         component = value\n",
    "#         sae = sae_dict[key]\n",
    "#         output = component.output\n",
    "#         nnsight_apply(assert_tuple, output)\n",
    "        \n",
    "#         mean_ablation_dict[key] = sae.encode(output[0]).mean(dim=0).save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27419/589361245.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_ablation_dict = torch.load(\"./mean_ablate.pt\")\n"
     ]
    }
   ],
   "source": [
    "mean_ablation_dict = torch.load(\"./mean_ablate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = ContrastiveDatasetBatch(full_dataset[-10:], tokenizer)\n",
    "\n",
    "d_sae = saes[0].cfg.d_sae\n",
    "seq_len = test_items.all_tokenized.shape[-1]\n",
    "\n",
    "assert seq_len == 65, \"sequence length is expected to be 65\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablated_model(tokenized, sae_mask_dict):\n",
    "\n",
    "    def ablated_sae(input, tokens, sae, mean_ablation, mask):\n",
    "        special_tokens_mask = torch.isin(tokens, torch.tensor(tokenizer.all_special_ids, dtype=torch.int64)) # true on special tokens (ie BOS)\n",
    "        sae_acts = sae.encode(input)\n",
    "        mean_input_diff = sae_acts - mean_ablation # add this to mean_ablation to get original value\n",
    "        masked_sae_acts = mean_ablation + mean_input_diff * mask # where mask=1, let the input pass through else mean ablate\n",
    "        masked_sae_acts[special_tokens_mask] *= 0 # zero out special tokens for clarity\n",
    "        sae_out = sae.decode(masked_sae_acts)\n",
    "        sae_out = sae_out.to(torch.float16)\n",
    "        sae_out[special_tokens_mask] = input[special_tokens_mask] # replace with non-sae acts on special toks\n",
    "        return sae_out, masked_sae_acts\n",
    "\n",
    "    sae_acts = {}\n",
    "    with wrapped_model.trace(tokenized):\n",
    "        for k,component in component_dict.items():\n",
    "            sae = sae_dict[k]\n",
    "            mean_ablation = mean_ablation_dict[k]\n",
    "            mask = sae_mask_dict[k]\n",
    "            output = component.output\n",
    "            nnsight_apply(assert_tuple, output)\n",
    "            \n",
    "            sae_input = output[0] # tensor inside tuple\n",
    "            sae_output, masked_sae_acts = ablated_sae(\n",
    "                input = sae_input, \n",
    "                tokens = tokenized,\n",
    "                sae = sae,\n",
    "                mean_ablation=mean_ablation,\n",
    "                mask=mask\n",
    "                )\n",
    "            sae_acts[k] = masked_sae_acts.save()\n",
    "            component.output = (sae_output,)\n",
    "        \n",
    "        output = wrapped_model.output.save()        \n",
    "\n",
    "    model_out = torch.topk(torch.softmax(output.logits[:, -1, :], dim=-1), k=3)\n",
    "\n",
    "    top_tokens = [tokenizer.convert_ids_to_tokens(model_out.indices[i]) for i in range(len(model_out.indices))]\n",
    "    top_values = model_out.values\n",
    "    return top_tokens, top_values, sae_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['1', '>>>', '2']],\n",
       " tensor([[0.8057, 0.0593, 0.0236]], device='cuda:0', dtype=torch.float16),\n",
       " {7: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       "  14: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       "  21: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       "  40: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_ablated_model_demo():\n",
    "    sae_mask_dict = {}\n",
    "    for k, v in sae_dict.items():\n",
    "        sae_mask_dict[k] = torch.rand(seq_len, d_sae)<0.9\n",
    "\n",
    "    tokens = test_items.all_tokenized[0:1]\n",
    "\n",
    "    return run_ablated_model(tokens, sae_mask_dict)\n",
    "run_ablated_model_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top tokens:\n",
      "{'1': 0.8466796875, '>>>': 0.044189453125, '2': 0.0139007568359375}\n",
      "sae acts:\n",
      "{7: {62: {10768: 16.308565139770508, 11635: 10.111810684204102}}, 14: {62: {1724: 4.361850738525391, 1788: 8.653759002685547, 2576: 23.2171688079834, 3805: 0.0, 4811: 7.84780740737915, 4834: 0.0, 6868: 0.0, 8269: 6.388430118560791, 8746: 6.1098222732543945, 9066: 6.905455589294434, 11766: 5.3273091316223145, 12929: 8.839942932128906, 15603: 6.792154788970947}, 63: {8746: 0.0}}, 21: {62: {534: 9.456023216247559, 6740: 15.843599319458008, 7015: 28.05893325805664, 11455: 0.0}, 63: {712: 0.0, 3076: 0.0, 5066: 0.0, 5880: 0.0, 8255: 0.0, 9551: 0.0, 10824: 0.0, 11416: 0.0, 12314: 8.729430198669434}, 64: {52: 0.0, 712: 0.0, 1197: 13.492095947265625, 1408: 0.0, 4351: 0.0, 6650: 0.0, 7192: 0.0, 8082: 7.36557674407959, 8127: 0.0, 9551: 0.0, 10003: 5.821122646331787, 12314: 4.424532413482666, 12598: 4.5804443359375, 13546: 3.8628344535827637, 14515: 0.0}}, 40: {64: {215: 0.0, 266: 13.017533302307129, 637: 39.54631042480469, 1073: 0.0, 1322: 0.0, 1435: 0.0, 2295: 12.03421401977539, 2493: 0.0, 2534: 13.144218444824219, 2664: 96.1795654296875, 2881: 0.0, 2930: 15.14702033996582, 2964: 0.0, 2996: 0.0, 3056: 26.018478393554688, 3685: 0.0, 3960: 22.06473159790039, 4501: 0.0, 4603: 16.230669021606445, 4689: 0.0, 4769: 0.0, 5862: 0.0, 6619: 0.0, 6742: 0.0, 7622: 17.65155792236328, 7792: 0.0, 8416: 0.0, 8778: 0.0, 9230: 22.126657485961914, 9309: 17.671791076660156, 9447: 29.770292282104492, 9682: 95.06075286865234, 10069: 0.0, 10155: 0.0, 10316: 18.90983009338379, 10628: 0.0, 10936: 0.0, 10993: 0.0, 11066: 90.84899139404297, 11103: 50.805389404296875, 11403: 0.0, 11579: 11.854168891906738, 11706: 0.0, 11839: 50.48057556152344, 12037: 12.242050170898438, 12258: 0.0, 12735: 0.0, 12988: 15.658918380737305, 13095: 0.0, 13113: 21.464136123657227, 13479: 29.657394409179688, 13967: 0.0, 14423: 59.594112396240234, 14504: 0.0, 15628: 15.645626068115234, 15705: 16.17450714111328, 15851: 0.0, 16039: 0.0}}}\n"
     ]
    }
   ],
   "source": [
    "def keep_dict_to_mask_tensor(keep_dict: dict, seq_len: int, d_sae: int) -> dict:\n",
    "    \"\"\"\n",
    "    Reconstruct the (seq_len x d_sae) mask tensors from a nested dict of\n",
    "    {layer_idx: { token_idx: [latent_idx1, latent_idx2, ...], ... }, ...}.\n",
    "\n",
    "    - keep_dict: the nested dict created by mask_tensor_to_keep_dict\n",
    "    - seq_len:   the sequence length (number of tokens)\n",
    "    - d_sae:     the latent dimension\n",
    "\n",
    "    Returns:\n",
    "      A dictionary {layer_idx -> (seq_len x d_sae) mask_tensor}.\n",
    "    \"\"\"\n",
    "    sae_mask_dict = {}\n",
    "\n",
    "    for layer_idx, layer_dict in keep_dict.items():\n",
    "        # Initialize a zero tensor for the mask\n",
    "        mask_tensor = torch.zeros(seq_len, d_sae, dtype=torch.float16)\n",
    "\n",
    "        # For each token_idx in that layer\n",
    "        for token_idx, latent_indices in layer_dict.items():\n",
    "            # For each latent dimension to keep\n",
    "            for latent_idx in latent_indices:\n",
    "                mask_tensor[token_idx, latent_idx] = 1.0\n",
    "\n",
    "        sae_mask_dict[layer_idx] = mask_tensor\n",
    "\n",
    "    return sae_mask_dict\n",
    "\n",
    "def mask_tensor_to_value_dict(\n",
    "    sae_mask_dict: dict, \n",
    "    discard_value: float = 0.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dictionary of {layer_idx -> (seq_len x d_sae) mask tensors}\n",
    "    into a nested dict specifying which token & latent dims do NOT match the\n",
    "    discard_value, and what those values are.\n",
    "\n",
    "    For each layer’s mask, we look for entries != discard_value.\n",
    "    Then we store them as:\n",
    "      {\n",
    "        layer_idx: {\n",
    "          token_idx: {\n",
    "            latent_idx: <mask_value>,\n",
    "            ...\n",
    "          },\n",
    "          ...\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    Args:\n",
    "        sae_mask_dict: Dict of {layer_idx -> mask_tensor}, each mask_tensor of shape [seq_len, d_sae].\n",
    "        discard_value: Any float value that should be treated as \"discard.\" Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        A nested dictionary of the structure described above, containing\n",
    "        all entries that are not equal to discard_value.\n",
    "    \"\"\"\n",
    "    value_dict = {}\n",
    "\n",
    "    for layer_idx, mask_tensor in sae_mask_dict.items():\n",
    "        # Find all positions where the mask is not the discard_value\n",
    "        keep_positions = (mask_tensor != discard_value).nonzero(as_tuple=False)\n",
    "\n",
    "        if keep_positions.shape[0] == 0:\n",
    "            # No entries to keep => store empty dict\n",
    "            value_dict[layer_idx] = {}\n",
    "            continue\n",
    "\n",
    "        layer_dict = {}\n",
    "        for token_idx, latent_idx in keep_positions:\n",
    "            token_idx = token_idx.item()\n",
    "            latent_idx = latent_idx.item()\n",
    "\n",
    "            # Get the actual mask value at that position\n",
    "            val = mask_tensor[token_idx, latent_idx].item()\n",
    "\n",
    "            if token_idx not in layer_dict:\n",
    "                layer_dict[token_idx] = {}\n",
    "            layer_dict[token_idx][latent_idx] = val\n",
    "\n",
    "        value_dict[layer_idx] = layer_dict\n",
    "\n",
    "    return value_dict\n",
    "\n",
    "\n",
    "\n",
    "def simple_run(\n",
    "    text: str,\n",
    "    latents_dict: dict,\n",
    "    requested_return_dict: dict,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Tokenize `text`.\n",
    "    2) Build a 'keep' mask dict => everything not mentioned is ablated (0).\n",
    "    3) Use run_ablated_model(...) with that mask.\n",
    "    4) Re-encode the final hidden states at each relevant layer to get the final latents\n",
    "       for exactly the positions we 'kept'.\n",
    "    5) Return top tokens, top probabilities, and final-latents dictionary.\n",
    "\n",
    "    Requirements:\n",
    "    - run_ablated_model(tokenized, sae_mask_dict) must be in scope.\n",
    "      * That function expects: 1 => pass original, 0 => ablate to mean.\n",
    "    \"\"\"\n",
    "\n",
    "    # # 1) Tokenize text (batch=1)\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    seq_len = tokenized.shape[1]\n",
    "    assert seq_len == 65, \"expected seq len 65 for circuit demo\"\n",
    "\n",
    "    # 2) Convert latents_dict => mask_tensors (1=keep, 0=ablate)\n",
    "    #    i.e. everything not in latents_dict is ablated (0)\n",
    "    sae_mask_dict = keep_dict_to_mask_tensor(\n",
    "        keep_dict=latents_dict,\n",
    "        seq_len=seq_len,\n",
    "        d_sae=d_sae,\n",
    "    )\n",
    "\n",
    "    # print(sae_mask_dict)\n",
    "\n",
    "\n",
    "    # 3) Run the ablated model\n",
    "    top_tokens, top_values, sae_acts = run_ablated_model(tokenized, sae_mask_dict)\n",
    "\n",
    "    # elementwise product of mask and sae_acts\n",
    "    saved_activations = {}\n",
    "    for k, v in sae_acts.items():\n",
    "        saved_activations[k] =  sae_acts[k][0]\n",
    "        sae_mask_dict[k] = sae_mask_dict[k].to(torch.bool)\n",
    "        saved_activations[k][~sae_mask_dict[k]] = -1\n",
    "    saved_activations = mask_tensor_to_value_dict(saved_activations, discard_value=-1)\n",
    "    \n",
    "\n",
    "    # squeeze the batch dim\n",
    "    top_tokens = top_tokens[0]\n",
    "    top_values = top_values[0]\n",
    "\n",
    "\n",
    "\n",
    "    top_tokens_dict = {}\n",
    "    for i, token in enumerate(top_tokens):\n",
    "        top_tokens_dict[token] = top_values[i].item()\n",
    "\n",
    "    return(top_tokens_dict, saved_activations)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dict_circuit = {7: {62: [10768, 11635]}, 14: {62: [1724, 1788, 2576, 3805, 4811, 4834, 6868, 8269, 8746, 9066, 11766, 12929, 15603], 63: [8746]}, 21: {62: [534, 6740, 7015, 11455], 63: [712, 3076, 5066, 5880, 8255, 9551, 10824, 11416, 12314], 64: [52, 712, 1197, 1408, 4351, 6650, 7192, 8082, 8127, 9551, 10003, 12314, 12598, 13546, 14515]}, 40: {64: [215, 266, 637, 1073, 1322, 1435, 2295, 2493, 2534, 2664, 2881, 2930, 2964, 2996, 3056, 3685, 3960, 4501, 4603, 4689, 4769, 5862, 6619, 6742, 7622, 7792, 8416, 8778, 9230, 9309, 9447, 9682, 10069, 10155, 10316, 10628, 10936, 10993, 11066, 11103, 11403, 11579, 11706, 11839, 12037, 12258, 12735, 12988, 13095, 13113, 13479, 13967, 14423, 14504, 15628, 15705, 15851, 16039]}}\n",
    "    top_tokens_dict, sae_acts = simple_run(test_items.correct_batch[0]['prompt'], dict_circuit, dict_circuit)\n",
    "    print(\"top tokens:\")\n",
    "    print(top_tokens_dict)\n",
    "    print(\"sae acts:\")\n",
    "    print(sae_acts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
